<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/css/images/octopus_favicon.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Lecture-7，介绍了进一步做优化的方法，比如给SGD加上动量以使其越过非极值点，还有最常应用的Adam算法；由对学习率的设置问题引出业界比较新颖的二阶优化的方法；而且，神经网络重点在于测试集上的效果，所以介绍了一些避免模型过拟合的正则化方法，如最常用的dropout；最后进一步介绍了迁移学习，可以将训练好的模型拿来针对自己的数据做一些调整使用">
<meta property="og:type" content="article">
<meta property="og:title" content="CS231n Lecture 7 Training Neural Networks Part II 进一步优化的方法 dropout正则化和迁移学习">
<meta property="og:url" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/index.html">
<meta property="og:site_name" content="Octopi">
<meta property="og:description" content="Lecture-7，介绍了进一步做优化的方法，比如给SGD加上动量以使其越过非极值点，还有最常应用的Adam算法；由对学习率的设置问题引出业界比较新颖的二阶优化的方法；而且，神经网络重点在于测试集上的效果，所以介绍了一些避免模型过拟合的正则化方法，如最常用的dropout；最后进一步介绍了迁移学习，可以将训练好的模型拿来针对自己的数据做一些调整使用">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Activation_Functions.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Weight_Initialization.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Data_Preprocessing.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Batch_Normalization.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Babysitting_Learning.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Hyperparameter_Search.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/SGD_problem_1.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/SGD_problem_2.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/SGD_problem_3.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/SGD_Momentum.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/SGD_Momentum_2.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Nesterov_Momentum.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Nesterov_Momentum_change.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/AdaGrad.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/RMSProp.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/RMSProp_adjust.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Adam_almost.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Adam.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Learningrate_decay.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Learningrate_decay_pic.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/FirstOrder_Optimization.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/SecondOrder_Optimization.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/nice_secondorder_optimization.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/L_BFGS.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Inpractice.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Beyond_TrainingError.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Model_Ensembles.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Model_Ensembles_Tricks_1.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Model_Ensembles_Tricks_2.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Dropout.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Testtime_dropout.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Dropout_Summary.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Inverted_dropout.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Regularization_common_pattern.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Regularization_DropConnect.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Regularization_Fractional_MaxPooling.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Regularization_Stochastic_Depth.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/TransferLearning_1.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/TransferLearning_2.png">
<meta property="og:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Summary.png">
<meta property="og:updated_time" content="2018-10-15T15:55:02.360Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CS231n Lecture 7 Training Neural Networks Part II 进一步优化的方法 dropout正则化和迁移学习">
<meta name="twitter:description" content="Lecture-7，介绍了进一步做优化的方法，比如给SGD加上动量以使其越过非极值点，还有最常应用的Adam算法；由对学习率的设置问题引出业界比较新颖的二阶优化的方法；而且，神经网络重点在于测试集上的效果，所以介绍了一些避免模型过拟合的正则化方法，如最常用的dropout；最后进一步介绍了迁移学习，可以将训练好的模型拿来针对自己的数据做一些调整使用">
<meta name="twitter:image" content="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Activation_Functions.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/"/>





  <title>CS231n Lecture 7 Training Neural Networks Part II 进一步优化的方法 dropout正则化和迁移学习 | Octopi</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Octopi</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Octopi Lau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Octopi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">CS231n Lecture 7 Training Neural Networks Part II 进一步优化的方法 dropout正则化和迁移学习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-15T23:55:02+08:00">
                2018-10-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Lecture-7，介绍了进一步做优化的方法，比如给SGD加上动量以使其越过非极值点，还有最常应用的Adam算法；由对学习率的设置问题引出业界比较新颖的二阶优化的方法；而且，神经网络重点在于测试集上的效果，所以介绍了一些避免模型过拟合的正则化方法，如最常用的dropout；最后进一步介绍了迁移学习，可以将训练好的模型拿来针对自己的数据做一些调整使用<br><a id="more"></a></p>
<h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><p>首先，课程回顾了一下之前所讲授的内容:</p>
<h4 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h4><p>关于激活函数，ReLU是常用的选择，不像sigmoid和tanH或有梯度弥散的问题<br><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Activation_Functions.png" alt="Activation Functions"></p>
<h4 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h4><p>权值的初始化呢，不能过大(梯度会成为0，不会学习)或者过小(激活值会变成0，梯度也会变成0)</p>
<p>使用正确的初始化方法，如Xavier initialization 或者 MSRA initialization</p>
<p>权值的初始化选取随着网络层级越来越深会变得愈发重要</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Weight_Initialization.png" alt="Weight Initialization"></p>
<h4 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h4><p>关于数据的预处理，在深度学历领域，中心化和归一化是常用的<br><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Data_Preprocessing.png" alt="Data Preprocessing"></p>
<h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>在传统网络中多加一层，以使得中间的激活值均值为0，方差为1</p>
<p>在正向传播的时候，使用小batch的均值和标准差，使用这个估值对整个数据进行归一化</p>
<p>这里2017年的slide写错了，我使用的2018年的slide</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Batch_Normalization.png" alt="Batch_Normalization"></p>
<h4 id="Babysitting-Learning"><a href="#Babysitting-Learning" class="headerlink" title="Babysitting Learning"></a>Babysitting Learning</h4><p>观察损失函数曲线，查看损失函数变化情况</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Babysitting_Learning.png" alt="Babysitting Learning"></p>
<h4 id="Hyperparameter-Search"><a href="#Hyperparameter-Search" class="headerlink" title="Hyperparameter Search"></a>Hyperparameter Search</h4><p>超参数搜索的方法，随即搜索比网格搜索具有优势</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Hyperparameter_Search.png" alt="Hyperparameter Search"></p>
<h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p>训练神经网络的核心，是优化问题</p>
<p>定义神经网络中的每个权值，使用损失函数界定这些权值是否恰当</p>
<h3 id="Problems-with-SGD"><a href="#Problems-with-SGD" class="headerlink" title="Problems with SGD"></a>Problems with SGD</h3><ol>
<li><p>当改变权值之一，损失值不敏感。当改变另外权值的时候，损失值却变得很敏感<br>这种问题在高纬度的情况下更加容易出现<br><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/SGD_problem_1.png" alt="Problems with SGD 1"></p>
</li>
<li><p>SGD的另外一个问题是当遭遇local minima(局部最小值)/saddle points(鞍点，非极大或极小值点)的时候，会卡住<br><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/SGD_problem_2.png" alt="Problems with SGD 2"><br>在一维问题中，局部最小值会是大问题。<br>但是在高维情况下，鞍点的问题会发生的极其频繁</p>
</li>
<li><p>SGD还有个问题是是stochastic(随机)，由于通过一个小batch来计算权值，会引入噪音，而SGD在噪音附近的计算可能会浪费大量资源<br><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/SGD_problem_3.png" alt="Problems with SGD 3"></p>
</li>
</ol>
<h3 id="SGD-Momentum-动量项-，解决以上问题"><a href="#SGD-Momentum-动量项-，解决以上问题" class="headerlink" title="SGD + Momentum(动量项)，解决以上问题"></a>SGD + Momentum(动量项)，解决以上问题</h3><p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/SGD_Momentum.png" alt="SGD + Momentum"></p>
<p>老师的解释非常有创意，使用物理的惯性规则讲解这个问题。</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/SGD_Momentum_2.png" alt="SGD + Momentum 2"></p>
<h3 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a>Nesterov Momentum</h3><p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Nesterov_Momentum.png" alt="Nesterov Momentum"></p>
<p>优化一下Nesterov</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Nesterov_Momentum_change.png" alt="Nesterov Momentum change"></p>
<h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/AdaGrad.png" alt="AdaGrad"></p>
<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>可以理解为给梯度的平方加动量，<br><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/RMSProp.png" alt="RMSProp"></p>
<p>可以看到相比于普通的SGD加动量，RMS可以一直调整<br><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/RMSProp_adjust.png" alt="RMSProp 2"></p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>为了逐步理解，首先介绍了一下，与Adam非常接近的算法</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Adam_almost.png" alt="Adam almost"></p>
<p>但是这样计算的话，在最初的第一步时，第二动量会一直非常小，会得到非常大的步长</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Adam.png" alt="Adam"></p>
<p>加上了Bias correction(偏置校正项)，避免过大的步长</p>
<p><strong>Adam在各种情况下表现都非常好</strong></p>
<h3 id="Learning-rate"><a href="#Learning-rate" class="headerlink" title="Learning rate"></a>Learning rate</h3><p>如何设置Learning rate？</p>
<p>有个小技巧是，使学习率随着时间衰减</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Learningrate_decay.png" alt="Learning rate decay over time"></p>
<p>从图示可以看出，在遇到平台期的时候，对学习率的衰减可以进一步降低损失</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Learningrate_decay_pic.png" alt="Learning rate decay PIC"></p>
<p>需要注意的是，在SGD+动量的时候，经常使用对学习率的衰减；而在Adam的时候很少用</p>
<p>还有，对学习率的衰减通常来说是个二阶超参是，不应该一开始就使用</p>
<p>先尝试不衰减，看看会发生什么，观察损失函数</p>
<h3 id="First-Order-Optimization-amp-Second-Order-Optimization"><a href="#First-Order-Optimization-amp-Second-Order-Optimization" class="headerlink" title="First-Order Optimization &amp; Second-Order Optimization"></a>First-Order Optimization &amp; Second-Order Optimization</h3><p>我们之前了解的优化算法都是First-Order Optimization(一阶优化算法)</p>
<p>一阶优化算法我们使用一阶梯度信息计算目标函数的线性逼近(这里面涉及到了一阶偏导)</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/FirstOrder_Optimization.png" alt="First-Order Optimization"></p>
<p>二阶逼近，同时考虑一阶梯度和二阶梯度信息，对函数做一个二阶泰勒逼近，实际上是使用二阶函数在局部逼近我们的目标函数</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/SecondOrder_Optimization.png" alt="Second-Order Optimization"></p>
<p>二阶优化让人惊讶之处在于: 没有超参数和学习率</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/nice_secondorder_optimization.png" alt="nice_secondorder_optimization"></p>
<p>从公式课件，二阶逼近需要求出Hessian(海森矩阵)的逆，因为Hessian是N×N的，N代表的是网络中的参数数量，这个N非常大，而转置这个Hessian将会更加巨大，所有这个理论的二阶逼近需要进一步提升</p>
<ol>
<li><p>Quasi-Newton methods (BGFS most popular) 拟牛顿法<br>逼近Hessian的逆而非求出它</p>
</li>
<li><p>L-BFGS (Limited memory BFGS)<br>不将Hessian的逆全部存储</p>
</li>
</ol>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/L_BFGS.png" alt="L-BFGS"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Inpractice.png" alt="In practice"></p>
<h2 id="Beyond-Training-Error"><a href="#Beyond-Training-Error" class="headerlink" title="Beyond Training Error"></a>Beyond Training Error</h2><p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Beyond_TrainingError.png" alt="Beyond Training Error"></p>
<p>我们更在意的是，在没见过的数据上，模型的表现</p>
<p>如何减少训练误差和测试误差才是我们需要的</p>
<h3 id="Model-Ensembles-模型集成"><a href="#Model-Ensembles-模型集成" class="headerlink" title="Model Ensembles(模型集成)"></a>Model Ensembles(模型集成)</h3><p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Model_Ensembles.png" alt="Model Ensembles"></p>
<p>可以固定的，提升几个百分点</p>
<p>一些模型集成的Tricks：</p>
<ol>
<li>保存单个模型训练中的快照<br><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Model_Ensembles_Tricks_1.png" alt="Model_Ensembles_Tricks_1"></li>
<li>Polyak averaging<br><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Model_Ensembles_Tricks_2.png" alt="Model_Ensembles_Tricks_2"></li>
</ol>
<h2 id="Regularization-正则化"><a href="#Regularization-正则化" class="headerlink" title="Regularization(正则化)"></a>Regularization(正则化)</h2><p>提升单一模型性能的办法——Regularization(正则化)</p>
<p>在模型中加入一些成分，防止模型在训练集上的过拟合</p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>除了前面课程介绍的L2正则化之外，Dropout也是一种在DL领域经常使用的正则化方法</p>
<p>在每次前向传播的过程中，在每层随机的将一部分神经元置零，然后在网络中继续前进</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Dropout.png" alt="dropout"></p>
<p>解释dropout有效的原因:</p>
<ol>
<li>Prevents co-adaptation of features(避免了特征间的相互适应)</li>
<li>可以把dropout看成是一种共享参数的网络做集成学习<br>由于每次dropout产生的子网络都不想同，但是共享的参数是相同的，所以可以做如上理解</li>
</ol>
<p>举例单个神经元，来说明在测试时的dropout产生的期望</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Testtime_dropout.png" alt="test time Dropout"></p>
<p>对dropout做个总结:</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Dropout_Summary.png" alt="Dropout Summary"></p>
<p>关于 Inverted dropout</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Inverted_dropout.png" alt="Inverted dropout"></p>
<h3 id="关于正则化的通用规则"><a href="#关于正则化的通用规则" class="headerlink" title="关于正则化的通用规则"></a>关于正则化的通用规则</h3><p>在训练时，给网络增加一些随机性，在一定程度上扰乱它，防止过拟化<br>在测试时，要抵消掉所有随机性，提升泛化能力</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Regularization_common_pattern.png" alt="common_pattern_regularization"></p>
<h3 id="Data-Augmentation-数据增强"><a href="#Data-Augmentation-数据增强" class="headerlink" title="Data Augmentation(数据增强)"></a>Data Augmentation(数据增强)</h3><p>使用一些方法对自己的原始数据做处理:</p>
<ol>
<li>Horizontal Flips(翻转)</li>
<li>Random crops and scales(随机抽取不同尺度的裁剪图像)</li>
<li>Color Jitter(色彩抖动)</li>
</ol>
<p>数据增强这种思想可以被拓展与各个DL问题的领域，在遇到问题的时候可以想一下新奇的办法，拓充自己的数据</p>
<h3 id="一些其他的正则化方法"><a href="#一些其他的正则化方法" class="headerlink" title="一些其他的正则化方法"></a>一些其他的正则化方法</h3><ol>
<li>DropConnect，不是每次将激活函数置零而是随机将权重矩阵的一些值置零<br><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Regularization_DropConnect.png" alt="DropConnect"></li>
<li>Fractional Max Pooling，部分做最大池化<br><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Regularization_Fractional_MaxPooling.png" alt="Fractional Max Pooling"></li>
<li>Stochastic Depth，简单来说是丢掉一些层<br><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Regularization_Stochastic_Depth.png" alt="Stochastic Depth"></li>
</ol>
<h2 id="Transfer-Learning-迁移学习"><a href="#Transfer-Learning-迁移学习" class="headerlink" title="Transfer Learning(迁移学习)"></a>Transfer Learning(迁移学习)</h2><p>使用正则化可以减小训练误差和测试误差之间的间隙</p>
<p>过拟合很多时候是由于数据不够，如果我们需要一个大的，泛化能力强的模型，在小数据集的时候很容易过拟合。</p>
<p>这时候我们可以使用正则化的方法，也可以选择迁移学习</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/TransferLearning_1.png" alt="Transfer Learning 1"></p>
<p>首先获取到，训练好的CNNs模型</p>
<p>然后我们需要使用这个由大数据集训练出的特征模型用在我们的小数据集上面，我们通常重新初始化最后一层的特征到最后的分类输出之间的全连接层这部分的矩阵。重新随机初始化最后的矩阵，冻结前面层的权重，只需要训练一个线性分类器(最后这层)，让最后这层在我们自己的数据上做收敛</p>
<p>当我们拥有的数据量更大一些的时候，我们可以选择调整整个网络，更新整个网络的权值</p>
<p>一个通用的规则是，在更新网络的时候，将学习率调低(由于整个网络可能是只需要微调以适应我们自己的数据)</p>
<p>当使用迁移学习时，可能遇到的情况:</p>
<p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/TransferLearning_2.png" alt="Transfer Learning 2"></p>
<p>数据量小而且与模型图片相似时(如选择resnet模型，图片与IMAGENET相似)，只训练最后一层</p>
<p>数据量较大而且与模型图片相似时，精调模型</p>
<p>但是当自己的数据与模型的数据不太相同的时候(比如，选择使用IMAGENET训练的resnet模型，但是手中的数据是CT图片什么的)</p>
<p>数据量小的时候比较麻烦，可能需要做大的调整</p>
<p>数据量大的时候对模型进行更精细的调整</p>
<p>各个DL软件的模型库:</p>
<p>caffe: <a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" target="_blank" rel="noopener">caffe</a><br>tensorflow: <a href="https://github.com/tensorflow/models" target="_blank" rel="noopener">tensorflow</a><br>pytorch: <a href="https://github.com/pytorch/vision" target="_blank" rel="noopener">pytorch</a></p>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p><img src="/2018/10/15/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/Summary.png" alt="Summary"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/" rel="next" title="CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置">
                <i class="fa fa-chevron-left"></i> CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Octopi Lau</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="http://github.com/stevelau94/" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#回顾"><span class="nav-number">1.</span> <span class="nav-text">回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Activation-Functions"><span class="nav-number">1.0.1.</span> <span class="nav-text">Activation Functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Weight-Initialization"><span class="nav-number">1.0.2.</span> <span class="nav-text">Weight Initialization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Data-Preprocessing"><span class="nav-number">1.0.3.</span> <span class="nav-text">Data Preprocessing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">1.0.4.</span> <span class="nav-text">Batch Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Babysitting-Learning"><span class="nav-number">1.0.5.</span> <span class="nav-text">Babysitting Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hyperparameter-Search"><span class="nav-number">1.0.6.</span> <span class="nav-text">Hyperparameter Search</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimization"><span class="nav-number">2.</span> <span class="nav-text">Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Problems-with-SGD"><span class="nav-number">2.1.</span> <span class="nav-text">Problems with SGD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SGD-Momentum-动量项-，解决以上问题"><span class="nav-number">2.2.</span> <span class="nav-text">SGD + Momentum(动量项)，解决以上问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Nesterov-Momentum"><span class="nav-number">2.3.</span> <span class="nav-text">Nesterov Momentum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaGrad"><span class="nav-number">2.4.</span> <span class="nav-text">AdaGrad</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RMSProp"><span class="nav-number">2.5.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam"><span class="nav-number">2.6.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-rate"><span class="nav-number">2.7.</span> <span class="nav-text">Learning rate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#First-Order-Optimization-amp-Second-Order-Optimization"><span class="nav-number">2.8.</span> <span class="nav-text">First-Order Optimization &amp; Second-Order Optimization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">2.9.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Beyond-Training-Error"><span class="nav-number">3.</span> <span class="nav-text">Beyond Training Error</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Ensembles-模型集成"><span class="nav-number">3.1.</span> <span class="nav-text">Model Ensembles(模型集成)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regularization-正则化"><span class="nav-number">4.</span> <span class="nav-text">Regularization(正则化)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout"><span class="nav-number">4.1.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关于正则化的通用规则"><span class="nav-number">4.2.</span> <span class="nav-text">关于正则化的通用规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Augmentation-数据增强"><span class="nav-number">4.3.</span> <span class="nav-text">Data Augmentation(数据增强)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一些其他的正则化方法"><span class="nav-number">4.4.</span> <span class="nav-text">一些其他的正则化方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transfer-Learning-迁移学习"><span class="nav-number">5.</span> <span class="nav-text">Transfer Learning(迁移学习)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-1"><span class="nav-number">6.</span> <span class="nav-text">总结</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Octopi Lau</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
