<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/css/images/octopus_favicon.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,深度学习,cs231n," />










<meta name="description" content="Lecture-6是介绍如何训练神经网络的第一部分，详细介绍了各种激活函数，并且介绍了神经网络领域中常用的数据预处理方法，以及一些在训练之前的权重初始化经验，超参数设置的经验和网络训练的流程。这节课还有个重要的知识点是Batch Normalization">
<meta name="keywords" content="Deep Learning,深度学习,cs231n">
<meta property="og:type" content="article">
<meta property="og:title" content="CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置">
<meta property="og:url" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/index.html">
<meta property="og:site_name" content="Octopi">
<meta property="og:description" content="Lecture-6是介绍如何训练神经网络的第一部分，详细介绍了各种激活函数，并且介绍了神经网络领域中常用的数据预处理方法，以及一些在训练之前的权重初始化经验，超参数设置的经验和网络训练的流程。这节课还有个重要的知识点是Batch Normalization">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Overview.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Part_1.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Activation_Functions.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Sigmoid公式.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Sigmoid图像.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/sigmoid_kill_gradients.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/sigmoid_anti_zero-centered.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/tanh(x">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/ReLU公式.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/ReLU图像.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/ReLU_dead.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Leaky_ReLU公式.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Leaky_ReLU图像.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/PReLU公式.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/ELU公式.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/ELU图像.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Maxout.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/TLDR_of_ActivationFunctions.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/DataPreprocessing.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/zero-mean.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/normalized.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/TLDR_of_DataPreprocessing.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Small_randomnumbers.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/activations_become_zero.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Big_randomnumbers.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/all_neurons_completely_saturated.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Xavier_initialization.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Xavier_initialization_good.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Xavier_initialization_ReLU.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Xavier_initialization_fix_ReLUproblem.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Xavier_initialization_fix_ReLUproblem_.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Batch_Normalization.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Batch_Normalization_.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Batch_Normalization_process.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/2_Search.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/learningh_rate.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/accuracy.png">
<meta property="og:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/TLDRs.png">
<meta property="og:updated_time" content="2019-01-06T15:03:20.063Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置">
<meta name="twitter:description" content="Lecture-6是介绍如何训练神经网络的第一部分，详细介绍了各种激活函数，并且介绍了神经网络领域中常用的数据预处理方法，以及一些在训练之前的权重初始化经验，超参数设置的经验和网络训练的流程。这节课还有个重要的知识点是Batch Normalization">
<meta name="twitter:image" content="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Overview.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/"/>





  <title>CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置 | Octopi</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Octopi</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Octopi Lau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Octopi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-01T15:33:05+08:00">
                2018-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/cs231n/" itemprop="url" rel="index">
                    <span itemprop="name">cs231n</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Lecture-6是介绍如何训练神经网络的第一部分，详细介绍了各种激活函数，并且介绍了神经网络领域中常用的数据预处理方法，以及一些在训练之前的权重初始化经验，超参数设置的经验和网络训练的流程。这节课还有个重要的知识点是Batch Normalization<br><a id="more"></a></p>
<h2 id="Training-Neural-Networks-Overview"><a href="#Training-Neural-Networks-Overview" class="headerlink" title="Training Neural Networks Overview"></a>Training Neural Networks Overview</h2><p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Overview.png" alt="Overview"></p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Part_1.png" alt="PART I"></p>
<h2 id="Activation-Functions-激活函数"><a href="#Activation-Functions-激活函数" class="headerlink" title="Activation Functions(激活函数)"></a>Activation Functions(激活函数)</h2><p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Activation_Functions.png" alt="Activation Functions"></p>
<h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>公式: </p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Sigmoid公式.png" alt="Sigmoid公式"></p>
<p>图像：</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Sigmoid图像.png" alt="Sigmoid图像"></p>
<p>效果:</p>
<p>每个元素被压缩到[0,1]之间</p>
<p>问题:</p>
<p><strong>Sigmoid导致梯度消失</strong>  </p>
<p>当输入数值很大的正值或者很大的负值的时候，会导致梯度成为0，从而无法得到梯度flow的反馈</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/sigmoid_kill_gradients.png" alt="“kill” the gradients"></p>
<p><strong>sigmoid是非零中心函数</strong></p>
<p>如果输入数值全部大于零或全部小于零，会导致梯度下降的非常低效(下降方向被固定住了)</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/sigmoid_anti_zero-centered.png" alt="zero-centered"></p>
<p><strong>存在exp()，计算成本高</strong> (不是很重要的一点，因为卷积运算才是大头)</p>
<h3 id="tanh-x"><a href="#tanh-x" class="headerlink" title="tanh(x)"></a>tanh(x)</h3><p>图像：</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/tanh(x" alt="tanh(x)图像">图像.png)</p>
<p>效果:</p>
<p>每个元素被压缩到[-1,1]之间</p>
<p>tanh(x)是zero-centered(零中心)函数</p>
<p>问题:</p>
<p><strong>导致梯度消失</strong>  (与sigmoid类似，从图像可以看出)</p>
<h3 id="ReLU-Rectified-Linear-Unit"><a href="#ReLU-Rectified-Linear-Unit" class="headerlink" title="ReLU (Rectified Linear Unit)"></a>ReLU (Rectified Linear Unit)</h3><p>公式: </p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/ReLU公式.png" alt="ReLU公式"></p>
<p>图像：</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/ReLU图像.png" alt="ReLU图像"></p>
<p>效果:</p>
<p>在正区域不会出现饱和的情况</p>
<p>计算高效(大概是sigmoid的6x)</p>
<p>与sigmoid相比更具有生物学上的合理性</p>
<p>问题:</p>
<p><strong>ReLU是非零中心函数</strong></p>
<p><strong>dead ReLU情况</strong><br>dead ReLU不会被激活和更新</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/ReLU_dead.png" alt="dead ReLU"></p>
<p>出现此情况的一些原因：</p>
<ol>
<li>初始化不好，权重设置的差</li>
<li>学习率太大的时候</li>
</ol>
<p>可以使用较小的正偏置来初始化ReLU，以增加其在初始化时被激活的可能性，并获得更新(但是是否有用存在争议，多数情况下还是设为0)</p>
<h3 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h3><p>公式: </p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Leaky_ReLU公式.png" alt="Leaky ReLU公式"></p>
<p>图像：</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Leaky_ReLU图像.png" alt="Leaky ReLU图像"></p>
<p>效果:</p>
<p>在正/负区域都不会出现饱和的情况</p>
<p>计算高效(大概是sigmoid的6x)</p>
<p><strong>没有dead ReLU情况</strong></p>
<h3 id="Parametric-Rectifier-PReLU"><a href="#Parametric-Rectifier-PReLU" class="headerlink" title="Parametric Rectifier (PReLU)"></a>Parametric Rectifier (PReLU)</h3><p>将alpha值当成一个学习参数而非设成固定值</p>
<p>公式: </p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/PReLU公式.png" alt="PReLU公式"></p>
<h3 id="Exponential-Linear-Units-ELU"><a href="#Exponential-Linear-Units-ELU" class="headerlink" title="Exponential Linear Units (ELU)"></a>Exponential Linear Units (ELU)</h3><p>公式: </p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/ELU公式.png" alt="ELU公式"></p>
<p>图像：</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/ELU图像.png" alt="ELU图像"></p>
<p>效果:</p>
<p>所有ReLU的有点ELU都有</p>
<p>实际上是介于ReLU和Leaky ReLU之间的</p>
<h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>公式: </p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Maxout.png" alt="Maxout公式"></p>
<p>效果:</p>
<p>没有开始的点乘操作</p>
<p>是泛化的ReLU 和 Leaky ReLU </p>
<p>线性机制的操作</p>
<p>不会饱和</p>
<p>没有’dead‘情况</p>
<p>问题:</p>
<p>参数数量会翻倍</p>
<h3 id="激活函数的使用经验"><a href="#激活函数的使用经验" class="headerlink" title="激活函数的使用经验"></a>激活函数的使用经验</h3><p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/TLDR_of_ActivationFunctions.png" alt="TLDR"></p>
<h2 id="Data-Preprocessing-数据预处理"><a href="#Data-Preprocessing-数据预处理" class="headerlink" title="Data Preprocessing(数据预处理)"></a>Data Preprocessing(数据预处理)</h2><h3 id="常见的处理方法"><a href="#常见的处理方法" class="headerlink" title="常见的处理方法"></a>常见的处理方法</h3><p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/DataPreprocessing.png" alt="Data Preprocessing"></p>
<h3 id="DL中只使用zero-mean-零均值化"><a href="#DL中只使用zero-mean-零均值化" class="headerlink" title="DL中只使用zero-mean(零均值化)"></a>DL中只使用zero-mean(零均值化)</h3><p>在DL中，主要就使用 <strong>zero-mean(零均值化)</strong> 处理就好了</p>
<ol>
<li>zero-mean data(零均值化)<br><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/zero-mean.png" alt="zero-mean"><br>如果不将数据零中心处理，获得的梯度都将是次最优的优化</li>
<li>normalized(归一化) DL不常用<br><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/normalized.png" alt="normalized"><br>通常归一化处理是在特征处于范围差别较大的情况下使用的，深度学习的特征一般都在一个小范围内，所以对归一化的使用不像一般的ML那么频繁</li>
</ol>
<p>还有ML中常见的其他的更复杂的预处理方法，DL中一般都不会使用，因为对于图像信息来说，不希望将这些信息映射到更低维的空间中。通常DL算法希望直接使用卷积运算并且得到原图像的空间结构。</p>
<h3 id="数据预处理的使用经验"><a href="#数据预处理的使用经验" class="headerlink" title="数据预处理的使用经验"></a>数据预处理的使用经验</h3><p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/TLDR_of_DataPreprocessing.png" alt="TLDR"></p>
<h2 id="Weight-Initialization-初始化权重值"><a href="#Weight-Initialization-初始化权重值" class="headerlink" title="Weight Initialization(初始化权重值)"></a>Weight Initialization(初始化权重值)</h2><h3 id="当把所有权重值都初始化为0-不使用"><a href="#当把所有权重值都初始化为0-不使用" class="headerlink" title="当把所有权重值都初始化为0(不使用)"></a>当把所有权重值都初始化为0(不使用)</h3><p>这样的话所有神经元都会做同样的事情，会有相同的梯度，都将会以相同的方式更新，这会导致所有神经元都是些相同的(而我们需要神经元学习到不同的东西)</p>
<p>所以不要用相同的权重值来初始化，这会导致无法打破参数对称问题</p>
<h3 id="所有权重都是小的随机数-不使用"><a href="#所有权重都是小的随机数-不使用" class="headerlink" title="所有权重都是小的随机数(不使用)"></a>所有权重都是小的随机数(不使用)</h3><p>假如用0.01乘标准高斯分布</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Small_randomnumbers.png" alt="Small_randomnumbers"></p>
<p>这种做法在浅层网络有效，但是在深层网络效果不好  </p>
<p>在深层网络中会导致无法持续激活</p>
<p>而且从反向传播来看，由于权值接近于零，会导致每层的梯度非常下，所以会导致基本上没什么更新</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/activations_become_zero.png" alt="activations_become_zero"></p>
<h3 id="所有权重都是大的随机数-不使用"><a href="#所有权重都是大的随机数-不使用" class="headerlink" title="所有权重都是大的随机数(不使用)"></a>所有权重都是大的随机数(不使用)</h3><p>假如用1乘标准高斯分布</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Big_randomnumbers.png" alt="Big_randomnumbers"></p>
<p>由于所有权值都很大，这会导致每层都处于饱和状态</p>
<p>而且所有梯队都会成为0，也会导致权值不会被更新</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/all_neurons_completely_saturated.png" alt="all_neurons_completely_saturated"></p>
<h3 id="Xavier-initialization-推荐"><a href="#Xavier-initialization-推荐" class="headerlink" title="Xavier initialization(推荐)"></a>Xavier initialization(推荐)</h3><p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Xavier_initialization.png" alt="Xavier_initialization"></p>
<p>这种初始化方法效果不错：</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Xavier_initialization_good.png" alt="Xavier_initialization_good"></p>
<p>存在的问题是，当使用ReLU作为激活函数时，会有问题</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Xavier_initialization_ReLU.png" alt="Xavier_initialization_ReLU"></p>
<h3 id="Xavier-initialization-fix-ReLU-problem-推荐"><a href="#Xavier-initialization-fix-ReLU-problem-推荐" class="headerlink" title="Xavier initialization fix ReLU problem(推荐)"></a>Xavier initialization fix ReLU problem(推荐)</h3><p>这好像是何凯明大大的论文！</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Xavier_initialization_fix_ReLUproblem.png" alt="Xavier_initialization_fix_ReLUproblem"></p>
<p>效果不错：</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Xavier_initialization_fix_ReLUproblem_.png" alt="Xavier_initialization_fix_ReLUproblem"></p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>在每一层batch(批量)激活，取目前batch处理的mean(均值)和方差来进行normalize(归一化)</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Batch_Normalization.png" alt="Batch Normalization"></p>
<p>在训练开始时设置这个值而不是权值初始化的时候，以便可以在每一层都获得较好的高斯分布，并在训练时可以一直保持。</p>
<p>假设我们在当前的batch中，有N个样本，维度是D:</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Batch_Normalization_.png" alt="Batch Normalization"></p>
<p>对每个维度都独立计算mean(均值)和variance(方差)</p>
<p>然后对其进行normalize(归一化)</p>
<p>Batch Normalization通常是在FC层或者Conv layer后，并且是在nonlinearity(非线性)处理之前插入的</p>
<p>在进行Batch Normalization之后，需要进行一次额外的缩放操作来恢复恒等函数。</p>
<p>整体的Batch Normalization过程:</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/Batch_Normalization_process.png" alt="Batch_Normalization_process"></p>
<p>批量归一化的好处:</p>
<ol>
<li>改进了网络整体的梯度流</li>
<li>提升了robust(鲁棒性)</li>
<li>可以再更广范围的学习率和不同的初始值下工作</li>
<li>可以作为一种正则化方法</li>
</ol>
<h2 id="Babysitting-the-Learning-Process"><a href="#Babysitting-the-Learning-Process" class="headerlink" title="Babysitting the Learning Process"></a>Babysitting the Learning Process</h2><p>Step 1: Preprocess the data<br>Step 2: Choose the architecture</p>
<p>接下来的部分，老师介绍了一个网络训练的流程<br>一些要点：</p>
<ol>
<li>disable regularizationr然后查看losss会否合理</li>
<li>crank up regularizationr然后查看loss是否如预期有所提升</li>
<li>Make sure that you can overfit very small portion of the training dataz在数据量较小的情况下，loss是否是0，换句话说accuracy能不能为1，就是看在小数据量的情况下，模型应该完美的拟合</li>
<li>loss not going down/Loss barely changing如果loss变化很小的话，可能是学习率设置的太小了</li>
<li>loss exploding的话，可能是学习率设置太大了</li>
<li>通常设置的学习率在[1e-3 … 1e-5]之间 learning rate we should be cross-validating is somewhere [1e-3 … 1e-5]</li>
</ol>
<h2 id="Hyperparameter-Optimization"><a href="#Hyperparameter-Optimization" class="headerlink" title="Hyperparameter Optimization"></a>Hyperparameter Optimization</h2><p>通常使用交叉验证来设置超参数或者学习率</p>
<h3 id="Cross-validation-strategy"><a href="#Cross-validation-strategy" class="headerlink" title="Cross-validation strategy"></a>Cross-validation strategy</h3><p>在训练集上面使用超参数，然后在验证集上观察这些超参数的实验效果</p>
<p>可以使用Random Search,在循环训练中，可以先初步的观察参数导致的交叉验证度(Coarse to fine search)，首先粗选一下，再进行细选。除了这两种筛选方法之外还可以使用Grid Search</p>
<p>Grid Search没有Random Search好</p>
<p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/2_Search.png" alt="2 type search"></p>
<h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/learningh_rate.png" alt="learningh_rate"></p>
<h3 id="准确率"><a href="#准确率" class="headerlink" title="准确率"></a>准确率</h3><p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/accuracy.png" alt="accuracy"></p>
<h2 id="TLDRs"><a href="#TLDRs" class="headerlink" title="TLDRs"></a>TLDRs</h2><p><img src="/2018/09/01/CS231n-Lecture-6-Training-Neural-Networks-Part-I-激活函数-数据预处理-BN和超参数设置/TLDRs.png" alt="TLDRs"></p>
<p><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf" target="_blank" rel="noopener">slides</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/cs231n/" rel="tag"># cs231n</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/31/CS231n-Lecture-5-Convolutional-Neural-Networks-卷积神经网络/" rel="next" title="CS231n-Lecture-5-Convolutional-Neural-Networks-卷积神经网络">
                <i class="fa fa-chevron-left"></i> CS231n-Lecture-5-Convolutional-Neural-Networks-卷积神经网络
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/09/07/CS231n-Lecture-7-Training-Neural-Networks-Part-II-进一步优化的方法-dropout正则化和迁移学习/" rel="prev" title="CS231n Lecture 7 Training Neural Networks Part II 进一步优化的方法 dropout正则化和迁移学习">
                CS231n Lecture 7 Training Neural Networks Part II 进一步优化的方法 dropout正则化和迁移学习 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Octopi Lau</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="http://github.com/stevelau94/" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-Neural-Networks-Overview"><span class="nav-number">1.</span> <span class="nav-text">Training Neural Networks Overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Activation-Functions-激活函数"><span class="nav-number">2.</span> <span class="nav-text">Activation Functions(激活函数)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Sigmoid"><span class="nav-number">2.1.</span> <span class="nav-text">Sigmoid</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tanh-x"><span class="nav-number">2.2.</span> <span class="nav-text">tanh(x)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ReLU-Rectified-Linear-Unit"><span class="nav-number">2.3.</span> <span class="nav-text">ReLU (Rectified Linear Unit)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Leaky-ReLU"><span class="nav-number">2.4.</span> <span class="nav-text">Leaky ReLU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parametric-Rectifier-PReLU"><span class="nav-number">2.5.</span> <span class="nav-text">Parametric Rectifier (PReLU)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exponential-Linear-Units-ELU"><span class="nav-number">2.6.</span> <span class="nav-text">Exponential Linear Units (ELU)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Maxout"><span class="nav-number">2.7.</span> <span class="nav-text">Maxout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#激活函数的使用经验"><span class="nav-number">2.8.</span> <span class="nav-text">激活函数的使用经验</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Preprocessing-数据预处理"><span class="nav-number">3.</span> <span class="nav-text">Data Preprocessing(数据预处理)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#常见的处理方法"><span class="nav-number">3.1.</span> <span class="nav-text">常见的处理方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DL中只使用zero-mean-零均值化"><span class="nav-number">3.2.</span> <span class="nav-text">DL中只使用zero-mean(零均值化)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据预处理的使用经验"><span class="nav-number">3.3.</span> <span class="nav-text">数据预处理的使用经验</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Weight-Initialization-初始化权重值"><span class="nav-number">4.</span> <span class="nav-text">Weight Initialization(初始化权重值)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#当把所有权重值都初始化为0-不使用"><span class="nav-number">4.1.</span> <span class="nav-text">当把所有权重值都初始化为0(不使用)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#所有权重都是小的随机数-不使用"><span class="nav-number">4.2.</span> <span class="nav-text">所有权重都是小的随机数(不使用)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#所有权重都是大的随机数-不使用"><span class="nav-number">4.3.</span> <span class="nav-text">所有权重都是大的随机数(不使用)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Xavier-initialization-推荐"><span class="nav-number">4.4.</span> <span class="nav-text">Xavier initialization(推荐)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Xavier-initialization-fix-ReLU-problem-推荐"><span class="nav-number">4.5.</span> <span class="nav-text">Xavier initialization fix ReLU problem(推荐)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">5.</span> <span class="nav-text">Batch Normalization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Babysitting-the-Learning-Process"><span class="nav-number">6.</span> <span class="nav-text">Babysitting the Learning Process</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hyperparameter-Optimization"><span class="nav-number">7.</span> <span class="nav-text">Hyperparameter Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cross-validation-strategy"><span class="nav-number">7.1.</span> <span class="nav-text">Cross-validation strategy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习率"><span class="nav-number">7.2.</span> <span class="nav-text">学习率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#准确率"><span class="nav-number">7.3.</span> <span class="nav-text">准确率</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TLDRs"><span class="nav-number">8.</span> <span class="nav-text">TLDRs</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Octopi Lau</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
